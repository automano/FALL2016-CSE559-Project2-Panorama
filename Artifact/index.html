<!DOCTYPE html>
<html>
<head>
<title>Project2</title>
</head>
<body>
<h1>Project 2: Panorama Mosaic Stitching</h1>
<h2>By Zheing Li 445612 <br><br>at 2016-10-05</h2>
<div style='text-align: left'>
<div>
<h3><b>1. Preparaion Work</b></h3>
<p>The very first job is to compute focal length. I use  <a href="http://www.vision.caltech.edu/bouguetj/calib_doc/index.html">Jean-yves' Calibration Toolbox</a> and follow those steps in  its guidance.</p>
<p>I take 16 photos for the cheakboard pattern from 16 different angle.<br><center><img alt="cheackbox" src="images/image001.jpg"></center></p>
<p>Then extract the grid corner. I set (Wintx,Winty) to (5,5), and size dx is 30mm, also dy is 30mm. One of 16 results is shown below.<br>
<table>
    <tr>
        <td>
            <img alt="corner1" src="images/image002.jpg">
        </td>
        <td>
            <img alt="corner1" src="images/image003.jpg">
        </td>
    </tr>
</table>
</p>
<p>Then call functions <i>Calibration</i> and <i>Recomp.corners</i>, I can get more precise focal length. I use this f as focal length and do distortion with coefficients k1 and k2.
<br>
<table>
        <tr>
            <td>
                <img style="width: 420px;height: 315px;" alt="calib1" src="images/image004.jpg">
            </td>
            <td>
                <img style="width: 420px;height: 315px;" alt="calib2" src="images/image005.jpg">
            </td>
            <td>
                <img style="width: 420px;height: 315px;" alt="calib3" src="images/image006.jpg">
            </td>
        </tr>
</table>
<br>The result data after calibration are<br>
</p>
<p style="margin-left:280px">
<br>Focal Length:          fc = [ <span style="color: red">951.53505</span>   1084.73918 ] uncertainty [ 14.62528   17.53058 ]<br>
<br>Principal point:       cc = [ 376.22201   176.93848 ] ? [ 23.84237   34.71087 ]<br>
<br>Skew:             alpha_c = [ 0.00000 ] ? [ 0.00000  ]   => angle of pixel axes = 90.00000 uncertainty 0.00000 degrees<br>
<br>Distortion:            kc = [ <span style="color: red">-0.26251   0.50053</span>  -0.01008   0.00897  0.00000 ] uncertainty [ 0.14172   1.32070   0.00634   0.00706  0.00000 ]<br>
<br>Pixel error:          err = [ 0.94628   0.67602 ]<br>
<br>Image size: 640*480<br>
</p>
<p><span style="color: red">Extra Credit</span>: This is the comparison between images with distortion and without distortion.<br>
<center><table>
    <tr>
        <td>
            <img alt="distortion" src="images/image001.jpg"><br><center>Before radial distortion</center><br>
        </td>
        <td>
            <img alt="nodistortion" src="images/image002.jpg"><br><center>After ridial distortion</center><br>
        </td>
    </tr>
</table>
</center>
</p>
</div>
<div>
<h3><b>2. Implementation Detail</b></h3>
<p>
(1) I use <a href="http://www.cs.ubc.ca/%7Elowe/keypoints/"> SIFT features</a>, becuase this algorithm is much more robust than another algorithms. But the features matcher I use is ratio feature matching I inplemented in Project1.
</p>
<p>(2) I implement <span style="color: green"><i>WarpSpherical.cpp</i></span> by using the principle from our leature. 
Convert the given spherical image coordinate into the corresponding planar image coordinate using the coordinate transformation equation from the lecture notes <a href="http://www.cse.wustl.edu/~furukawa/cse559a/2016_fall/notes/06-1-stitching.pdf">06-1-stitching Page25 (Spherical Warping)</a>. 
Apply radial distortion using the equation from the lecture notes <a href="http://www.cse.wustl.edu/~furukawa/cse559a/2016_fall/notes/05-cameras-3.pdf">05-cameras-3 Page50 (Modeling Radial Distortion)</a>.<br>
<center>
<table>
    <tr>
        <td>
            <img style="width: 640px;height: 480px;" alt="stitching" src="images/06-1-stitching.jpg">
        </td>
        <td>
            <img style="width: 640px;height: 480px;" alt="distortion" src="images/05-cameras-3.jpg">
        </td>
    </tr>
</table>
</center>

</p>
<p>
(3) Then I implement <span style="color: green"><i>FeatureAlign.cpp</i></span> to compute the alignment of two images. 
<span style="color: green"><i>AlignPair</i></span> takes two feature sets, f1 and f2, the list of feature matches obtained from the SIFT features, a motion model, and estimates and inter-image transform matrix M. 
For this project, motion model is only translation which just have two freedom.
<span style="color: green"><i>AlignPair</i></span> uses RANSAC (RAndom SAmpling Consensus) to pull out a minimal set of feature matches, estimates the corresponding motion and then invokes countInliers to count how many of the feature matches agree with the current motion estimate. 
After repeated trials, the motion estimate with the largest number of inliers is used to compute a least squares estimate for the motion, which is then returned in the motion estimate M.
<span style="color: green"><i>CountInliers</i></span> computes the number of matches that have a distance below RANSACthresh is computed. It also returns a list of inlier match ids.
<span style="color: green"><i>LeastSquaresFit</i></span> computes a least squares estimate for the translation using all of the matches previously estimated as inliers. It returns the resulting translation estimate in the last column of M. 
Below is the comparison of my pairlist of yosemite images set with the pairlist project2 alreay provided.
<center>
	<img alt="mypairlist" src="images/">
	
	<img alt="projectpairlist" src="images/">
</center>
</p>
</div>
</div>
</body>
</html>